---
title: "Rethinking_Stats_Chapter_12.Rmd"
author: "Alex Bayers"
date: "04/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Easy

### Problem 1

Prior a) will produce more shrinkage, as the lower standard deviation forces each a(tank) closer to 0.

### Problem 2
$$
\\y_i \sim  Binomial(1, p_i)
\\logit(p_i) \sim \alpha_{GROUP[i]} \> + \> \beta x_i
\\\alpha_{GROUP} \sim Normal(\alpha, \>\sigma)
\\\alpha \sim Normal(0, 10)
\\\beta \sim Normal(0, 1)
\\\sigma \sim HalfCauchy(0, 1)
$$

### Problem 3
$$
\\y_i \sim  Normal(\mu_i, \sigma)
\\\mu_i \sim \alpha_{GROUP[i]} \> + \> \beta x_i
\\\alpha_{GROUP} \sim Normal(\alpha, \>\epsilon)
\\\alpha \sim Normal(0, 10)
\\\beta \sim Normal(0, 1)
\\\sigma \sim HalfCauchy(0, 2)
\\\epsilon \sim HalfCauchy(0, 2)
$$

### Problem 4
$$
\\T_i \sim Poisson(\mu_i)
\\log(u_i) \sim \alpha + \alpha_{SOCIETY_{[i]}} + \beta_P * log \> P_i
\\\alpha \sim Normal(0, 10)
\\\beta_P \sim Normal(0, 1)
\\\alpha_{SOCIETY} \sim Normal(0, \sigma_{SOCIETY})
\\\sigma_{SOCIETY} \sim HalfCauchy(0,1)
$$

# Medium
### Problem 1
```{r}
library(rethinking)
data(reedfrogs)
d <- reedfrogs
d$tank <- 1:nrow(d)

m12.1 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank]  ~ dnorm(0, 5)
  ),
  data = d
)

precis(m12.1, depth = 2)
```

```{r, warning=FALSE}
m12.2 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank]  ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ),
  data = d,
  iter = 4000,
  chains = 4
)

precis(m12.2, depth = 2)
```

```{r, warning=FALSE}
compare(m12.1, m12.2)
```

```{r, warning=FALSE}
#extract STAN samples
post <- extract.samples(m12.2)

d$propsurv.est <- logistic(apply(post$a_tank, 2, median))

#display raw proportions
plot(d$propsurv, ylim = c(0, 1), pch = 16, xaxt = "n", xlab = "tank", ylab= "proportion survival", col = rangi2)
axis(1, at = c(1, 16, 32, 48), labels = c(1, 16, 32, 48))

#overlay posterior medians
points(d$propsurv.est)

#mark posterior median probability across tanks
abline(h=logistic(median(post$a)), lty = 2)

#draw vertical dividers
abline(v=16.5, lwd = 0.5)
abline(v=32.5, lwd = 0.5)
text(8, 0, "small tanks")
text(16+8, 0, "medium tanks")
text(32+8, 0, "large tanks")
```


```{r, warning=FALSE}
d$predbool <- ifelse(d$pred == "pred", 1, 0)
m12.3 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank] + bp*predbool,
    a_tank[tank]  ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1),
    bp ~ dnorm(0, 10)
  ),
  data = d,
  iter = 4000,
  chains = 4
)

precis(m12.3, depth = 2)

```

```{r, warning=FALSE}
#Now ordered logit
d$large = ifelse(d$size=="large", 1, 0)
d$medium = ifelse(d$size=="medium", 1, 0)

m12.4 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank] +  bl*large + bm*medium,
    a_tank[tank]  ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1),
    bp ~ dnorm(0, 10),
    bl ~ dnorm(0, 10),
    bm ~ dnorm(0, 10)
  ),
  data = d,
  iter = 4000,
  chains = 4
)

precis(m12.4, depth = 2)

```


```{r, warning=FALSE}
#Now ordered logit
d$large = ifelse(d$size=="large", 1, 0)
d$medium = ifelse(d$size=="medium", 1, 0)

m12.5 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank] + bp*predbool + bl*large + bm*medium,
    a_tank[tank]  ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1),
    bp ~ dnorm(0, 3),
    bl ~ dnorm(0, 3),
    bm ~ dnorm(0, 3)
  ),
  data = d,
  iter = 4000,
  chains = 4
)

precis(m12.5, depth = 2)
```

```{r}
#Now interactions
m12.6 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank] + bp*predbool + bl*large + bm*medium + bpl*predbool*large + bpm*predbool*medium,
    a_tank[tank]  ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1),
    bp ~ dnorm(0, 10),
    bl ~ dnorm(0, 3),
    bm ~ dnorm(0, 3),
    bpl ~ dnorm(0, 3),
    bpm ~ dnorm(0, 3)
  ),
  data = d,
  iter = 4000,
  chains = 4
)

precis(m12.6, depth = 2)
```


```{r}
compare(m12.1, m12.2, m12.3, m12.4, m12.5, m12.6)
coeftab(m12.3, m12.5, m12.6, se = TRUE)
```


Looking at the model, the addition of predation substantially shrinks the variance (sigma) of a(t).  Effectively, our parameters were overdispersed because of the omitted variable bias.


###Problem 2
Interestingly, despite quite a wide standard error bar on the coefficients in models 12.5 and 12.6, the difference in WAIC is not very large between models 12.3, 12.5, and 12.6.  Looking at the effective number of parameters, there are scarcely any more free parameters in m12.5 versus m12.6 - (28.7 and 28.8 versus 28.6).  I think there are fewer effective parameters because of the large number of varying coefficients - by adding additional shrinkage from the new variables this can easily offset the additional variables.


###Problem 3
```{r}
m12.7 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank]  ~ dcauchy(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ),
  data = d,
  iter = 6000,
  chains = 4,
  control = list(max_treedepth=15)
)

precis(m12.7, depth = 2)

```


 The outcomes have substantially higher dispersion and less shrinkage.  This reflects two facts - one, in logistic regression, large values are both very close to zero; two, that 


```{r}
post.new <- extract.samples(m12.7)

d$propsurv.new <- logistic(apply(post.new$a_tank, 2, median))

#display normal proportions
plot(d$propsurv.est, ylim = c(0, 1), pch = 16, xaxt = "n", xlab = "tank", ylab= "proportion survival", col = rangi2)
axis(1, at = c(1, 16, 32, 48), labels = c(1, 16, 32, 48))

#overlay cauchy medians
points(d$propsurv.new)

#mark posterior median probability across tanks
abline(h=logistic(median(post$a)), lty = 2)

#draw vertical dividers
abline(v=16.5, lwd = 0.5)
abline(v=32.5, lwd = 0.5)
text(8, 0, "small tanks")
text(16+8, 0, "medium tanks")
text(32+8, 0, "large tanks")

```


###Problem 4
```{r}
data("chimpanzees")
d <- chimpanzees
d$block_id <- d$block

m12.8 <- map2stan(
  alist(
    pulled_left <- dbinom(1, p),
    logit(p) <- a_actor[actor] + a_block[block_id] + (bp+ bpC*condition)*prosoc_left,
    a_actor[actor] ~ dnorm(alpha, sigma_actor),
    a_block[block_id] ~ dnorm(gamma, sigma_block),
    c(alpha, gamma, bp, bpC) ~ dnorm(0, 10),
    c(sigma_actor, sigma_block) ~ dcauchy(0, 1)
  ),
  data = d,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  cores = 4
)

precis(m12.8, depth = 2)

```

The chains are estimated quote poorly; in particular, with low n_eff, because of the high correlation between the two different grand mean parameters.

```{r}
plot(precis(m12.8, depth = 2))
plot(m12.8)
```

#Hard
###Problem 1
```{r}
data("bangladesh")
d <- bangladesh
d$district_id <- as.integer(as.factor(d$district))

m12.9 <- map2stan(
  alist(
    use.contraception <- dbinom(1, p),
    logit(p) <- a_district[district_id],
    a_district[district_id] ~ dnorm(alpha, sigma_district),
    alpha ~ dnorm(0, 10),
    sigma_district ~ dcauchy(0, 1)
  ),
  data = d,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  cores = 4
)

precis(m12.9, depth = 2)
```

```{r}
m12.10 <- map2stan(
  alist(
    use.contraception <- dbinom(1, p),
    logit(p) <- a_district[district_id],
    a_district[district_id] ~ dnorm(0, 10)
  ),
  data = d,
  warmup = 1000,
  iter = 5000,
  chains = 4,
  cores = 4
)

precis(m12.10, depth = 2)


post.9 <- extract.samples(m12.9)
post.10 <- extract.samples(m12.10)


plot(logistic(apply(post.9$a_district, 2, median)), col=rangi2, pch =16, ylim = c(0, 1), ylab="")
points(logistic(apply(post.10$a_district, 2, median)))

```
There's substantial shrinkage, particularly in the cases near 0 and near 1.

###Problem 2
```{r}
data(Trolley)
d <- Trolley
d$factor_id <- as.integer(as.factor(d$id))

m12.11 <- map2stan(
  alist(
    response ~ dordlogit( phi , cutpoints ),
    phi <- bA*action + bC*contact + bI*intention + intercept[factor_id],
    cutpoints ~ dnorm(0,10),
    c(bA, bC, bI) ~ dnorm(0, 10),
    intercept[factor_id] ~ dnorm(0, sigma_intercept),
    sigma_intercept ~ dcauchy(0, 1)
  ),
  data=d ,
  start=list(cutpoints=c(-2,-1,0,1,2,2.5)),
  chains = 2,
  cores = 2
)

precis(m12.11, depth = 2)
```

    #phi <- bA*Action + bI*Intention + bC*Contact + a_individual[factor_id] + intercept,
    #a_individual[factor_id] ~ dnorm(0, sigma),
    #c(bA, bI, bC, intercept) ~ dnorm(0, 10),
    #sigma ~ dcauchy(0, 1),
    
```{r}
m12.12 <- map2stan(
  alist(
    response ~ dordlogit( phi , cutpoints ),
    phi <- bA*action + bC*contact + bI*intention,
    cutpoints ~ dnorm(0,10),
    c(bA, bC, bI) ~ dnorm(0, 10)
  ),
  data=d ,
  start=list(cutpoints=c(-2,-1,0,1,2,2.5)),
  chains = 2,
  cores = 2
)

precis(m12.12, depth = 2)
```
```{r}
compare(m12.11, m12.12)

```

The varying intercept model fits much better and has a much lowre WAIC, despite the higher degrees of freedom.

###Problem 3
```{r}
d$story_id <- as.integer(as.factor(d$story))

m12.13 <- map2stan(
  alist(
    response ~ dordlogit( phi , cutpoints ),
    phi <- bA*action + bC*contact + bI*intention + a_individual[factor_id] + a_story[story_id],
    cutpoints ~ dnorm(0,10),
    c(bA, bC, bI) ~ dnorm(0, 10),
    a_individual[factor_id] ~ dnorm(0, sigma_individual),
    a_story[story_id] ~ dnorm(0, sigma_story),
    sigma_individual ~ dcauchy(0, 1),
    sigma_story ~ dcauchy(0, 1)
  ),
  data=d ,
  start=list(cutpoints=c(-2,-1,0,1,2,2.5)),
  chains = 2,
  cores = 2
)

precis(m12.13, depth = 2)
```

```{r}
compare(m12.11, m12.12, m12.13)
```
