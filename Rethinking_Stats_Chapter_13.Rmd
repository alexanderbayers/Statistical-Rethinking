---
title: "Rethinking_Stats_Chapter_13.Rmd"
author: "Alex Bayers"
date: "13/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Easy

### Problem 1
$$
\\y_i \sim  Normal(\mu_i, \sigma)
\\\mu_i \sim \alpha_{GROUP[i]} \> + \> \beta_{GROUP[i]} x_i
\\\begin{equation*}
\left[
  \begin{array}{ccc}
  \alpha_{group} \\
  \beta_{group}\\
  \end{array} \right]
\sim MVNormal \bigg(
  \left[
    \begin{array}{ccc}
    \alpha \\
    \beta \\
    \end{array} \right],
  \textbf{S} \bigg)
\\
\textbf{S} =
  \left[
    \begin{array}{ccc}
    \sigma_a & 0 \\
    0 & \sigma_b\\
    \end{array}
    \right]
\textbf{R}
\left[
  \begin{array}{ccc}
  \sigma_a & 0 \\
  0 & \sigma_b\\
  \end{array}
  \right]
\\\alpha \sim Normal(0, 10)
\\\beta \sim Normal(0, 1)
\\(\sigma_a, \sigma_b) \sim HalfCauchy(0, 2)
\\\textbf{R} \sim LKJcorr(2)
\end{equation*}
$$


###Problem 2
A busy restaurant may be even busier at dinner than it is at lunch.  In this case, the dinner indicator variable may positively covary with a higher lunch intercept.

###Problem 3
The varying slopes model can have fewer effective parameters when the regularization helps shrink the parameter space.

##Medium
###Problem 1
```{r}
library(rethinking)
## R code 13.1
a <- 3.5            # average morning wait time
b <- (-1)           # average difference afternoon wait time
sigma_a <- 1        # std dev in intercepts
sigma_b <- 0.5      # std dev in slopes
rho <- 0      # correlation between intercepts and slopes

## R code 13.2
Mu <- c( a , b )

## R code 13.3
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix( c(sigma_a^2,cov_ab,cov_ab,sigma_b^2) , ncol=2 )

## R code 13.4
matrix( c(1,2,3,4) , nrow=2 , ncol=2 )

## R code 13.5
sigmas <- c(sigma_a,sigma_b) # standard deviations
Rho <- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix

# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)

## R code 13.6
N_cafes <- 20

## R code 13.7
library(MASS)
set.seed(5) # used to replicate example
vary_effects <- mvrnorm( N_cafes , Mu , Sigma )

## R code 13.8
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]

## R code 13.9
plot( a_cafe , b_cafe , col=rangi2 ,
    xlab="intercepts (a_cafe)" , ylab="slopes (b_cafe)" )

# overlay population distribution
library(ellipse)
for ( l in c(0.1,0.3,0.5,0.8,0.99) )
    lines(ellipse(Sigma,centre=Mu,level=l),col=col.alpha("black",0.2))

## R code 13.10
set.seed(500)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep( 1:N_cafes , each=N_visits )
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- 0.5  # std dev within cafes
wait <- rnorm( N_visits*N_cafes , mu , sigma )
d <- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait )

## R code 13.12
m13.1 <- map2stan(
    alist(
        wait ~ dnorm( mu , sigma ),
        mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
        c(a_cafe,b_cafe)[cafe] ~ dmvnorm2(c(a,b),sigma_cafe,Rho),
        a ~ dnorm(0,10),
        b ~ dnorm(0,10),
        sigma_cafe ~ dcauchy(0,2),
        sigma ~ dcauchy(0,2),
        Rho ~ dlkjcorr(2)
    ) ,
    data=d ,
    iter=5000 , warmup=2000 , chains=2 )

## R code 13.13
post <- extract.samples(m13.1)
dens( post$Rho[,1,2] )
```

The posterior distribution is centered around zero for rho and the ellipses are symmetrical.

###Problem 2
```{r}
m13.2 <- map2stan(
    alist(
        wait ~ dnorm( mu , sigma_cafe),
        mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
        a_cafe[cafe] ~ dnorm(a, sigma_a),
        b_cafe[cafe] ~ dnorm(b, sigma_b),
        a ~ dnorm(0,10),
        b ~ dnorm(0,10),
        sigma_cafe ~ dcauchy(0,1),
        sigma_a ~ dcauchy(0,1),
        sigma_b ~ dcauchy(0, 1)
    ) ,
    data=d ,
    iter=5000 , warmup=2000 , chains=2 )

post2 <- extract.samples(m13.2)

compare(m13.1, m13.2)
coeftab(m13.1, m13.2)

```

The models do an essentially identical job of fitting the data (WAIC weights are 50-50) when the correlation is zero, as this matches the generative process.  The one extra variable coming from the correlation is offset by additional regularization.  When we set a correlation to -.7, the model allowing nonzero correlation fits better.  The effective number of parameters is the same, but the fit with the nonzero correlation is better.

###Problem 3
```{r}
data(UCBadmit)
d <- UCBadmit
d$male <- ifelse( d$applicant.gender=="male" , 1 , 0 )
d$dept_id <- coerce_index( d$dept )

## R code 13.19
m13.3 <- map2stan(
    alist(
        admit ~ dbinom( applications , p ),
        logit(p) <- a + bm*male + a_dept[dept_id] +
                    bm_dept[dept_id]*male,
        c(a_dept,bm_dept)[dept_id] ~ dmvnorm2( c(0,0) , sigma_dept , Rho ),
        a ~ dnorm(0,10),
        bm ~ dnorm(0,1),
        sigma_dept ~ dcauchy(0,2),
        Rho ~ dlkjcorr(2)
    ) ,
    data=d , warmup=1000 , iter=5000 , chains=4 , cores=3 )

## R code 13.20
plot( precis(m13.3,pars=c("a_dept","bm_dept"),depth=2) )

summary(m13.3)
```

```{r}
m13.3nc1 <- map2stan(
    alist(
        admit ~ dbinom( applications , p ),

        # linear models
        logit(p) <- A + BM*male,
        A <- a + a_dept[dept_id],
        BM <- bm + bm_dept[dept_id],
        
        # adaptive priors
        c(a_dept,bm_dept)[dept_id] ~ dmvnormNC(sigma_dept, Rho),

        # fixed priors
        a ~ dnorm(0,10),
        bm ~ dnorm(0, 1),
        sigma_dept ~ dcauchy(0,2),
        Rho ~ dlkjcorr(2)
    ) ,
    data=d ,
    iter=5000 , warmup=1000 , chains=4 , cores=3 )

summary(m13.3nc1)



```

```{r}
# extract n_eff values for each model
neff_c <- precis(m13.3,2)@output$n_eff
neff_nc <- precis(m13.3nc1,2)@output$n_eff
# plot distributions
boxplot( list( 'm13.3'=neff_c , 'm13.3NC'=neff_nc ) ,
    ylab="effective samples" , xlab="model" )
compare(m13.3, m13.3nc1)
```

The noncentered model samples better (higher neff, and faster time per sample), generates similar standard errors, and has the same WAIC.  

###Problem 4
```{r}
## R code 13.29
# load the distance matrix
library(rethinking)
data(islandsDistMatrix)

# display short column names, so fits on screen
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
round(Dmat,1)

## R code 13.30
# linear
curve( exp(-1*x) , from=0 , to=4 , lty=2 ,
    xlab="distance" , ylab="correlation" )

# squared
curve( exp(-1*x^2) , add=TRUE )

## R code 13.31
data(Kline2) # load the ordinary data, now with coordinates
d <- Kline2
d$society <- 1:10 # index observations

m13.7 <- map2stan(
    alist(
        total_tools ~ dpois(lambda),
        log(lambda) <- a + g[society] + bp*logpop,
        g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),
        a ~ dnorm(0,10),
        bp ~ dnorm(0,1),
        etasq ~ dcauchy(0,1),
        rhosq ~ dcauchy(0,1)
    ),
    data=list(
        total_tools=d$total_tools,
        logpop=d$logpop,
        society=d$society,
        Dmat=islandsDistMatrix),
    warmup=2000 , iter=1e4 , chains=4 )

## R code 13.32
precis(m13.7,depth=2)

## R code 13.33
post <- extract.samples(m13.7)

# plot the posterior median covariance function
curve( median(post$etasq)*exp(-median(post$rhosq)*x^2) , from=0 , to=10 ,
    xlab="distance (thousand km)" , ylab="covariance" , ylim=c(0,1) ,
    yaxp=c(0,1,4) , lwd=2 )

# plot 100 functions sampled from posterior
for ( i in 1:100 )
    curve( post$etasq[i]*exp(-post$rhosq[i]*x^2) , add=TRUE ,
        col=col.alpha("black",0.2) )

## R code 13.34
# compute posterior median covariance among societies
K <- matrix(0,nrow=10,ncol=10)
for ( i in 1:10 )
    for ( j in 1:10 )
        K[i,j] <- median(post$etasq) *
                  exp( -median(post$rhosq) * islandsDistMatrix[i,j]^2 )
diag(K) <- median(post$etasq) + 0.01

## R code 13.35
# convert to correlation matrix
Rho <- round( cov2cor(K) , 2 )
# add row/col names for convenience
colnames(Rho) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
rownames(Rho) <- colnames(Rho)
Rho

## R code 13.36
# scale point size to logpop
psize <- d$logpop / max(d$logpop)
psize <- exp(psize*1.5)-2

# plot raw data and labels
plot( d$lon2 , d$lat , xlab="longitude" , ylab="latitude" ,
    col=rangi2 , cex=psize , pch=16 , xlim=c(-50,30) )
labels <- as.character(d$culture)
text( d$lon2 , d$lat , labels=labels , cex=0.7 , pos=c(2,4,3,3,4,1,3,2,4,2) )

# overlay lines shaded by Rho
for( i in 1:10 )
    for ( j in 1:10 )
        if ( i < j )
            lines( c( d$lon2[i],d$lon2[j] ) , c( d$lat[i],d$lat[j] ) ,
                lwd=2 , col=col.alpha("black",Rho[i,j]^2) )

## R code 13.37
# compute posterior median relationship, ignoring distance
logpop.seq <- seq( from=6 , to=14 , length.out=30 )
lambda <- sapply( logpop.seq , function(lp) exp( post$a + post$bp*lp ) )
lambda.median <- apply( lambda , 2 , median )
lambda.PI80 <- apply( lambda , 2 , PI , prob=0.8 )

# plot raw data and labels
plot( d$logpop , d$total_tools , col=rangi2 , cex=psize , pch=16 ,
    xlab="log population" , ylab="total tools" )
text( d$logpop , d$total_tools , labels=labels , cex=0.7 ,
    pos=c(4,3,4,2,2,1,4,4,4,2) )

# display posterior predictions
lines( logpop.seq , lambda.median , lty=2 )
lines( logpop.seq , lambda.PI80[1,] , lty=2 )
lines( logpop.seq , lambda.PI80[2,] , lty=2 )

# overlay correlations
for( i in 1:10 )
    for ( j in 1:10 )
        if ( i < j )
            lines( c( d$logpop[i],d$logpop[j] ) ,
                   c( d$total_tools[i],d$total_tools[j] ) ,
                   lwd=2 , col=col.alpha("black",Rho[i,j]^2) )


```

```{r}
data(Kline)
new_d <- Kline
new_d$log_pop <- log(new_d$population)
new_d$contact_high <- ifelse( new_d$contact=="high" , 1 , 0 )

m10.10 <- map2stan(
    alist(
        total_tools ~ dpois( lambda ),
        log(lambda) <- a + bp*log_pop +
            bc*contact_high + bpc*contact_high*log_pop,
        a ~ dnorm(0,100),
        c(bp,bc,bpc) ~ dnorm(0,1)
    ),
    data=new_d,
    warmup=1000 , iter=5000 , chains=4)

m10.11 <- map2stan(
    alist(
        total_tools ~ dpois( lambda ),
        log(lambda) <- a + bp*log_pop + bc*contact_high,
        a ~ dnorm(0,100),
        c(bp,bc) ~ dnorm( 0 , 1 )
    ), data=new_d,
    warmup=1000 , iter=5000 , chains=4)

m10.12 <- map2stan(
    alist(
        total_tools ~ dpois( lambda ),
        log(lambda) <- a + bp*log_pop,
        a ~ dnorm(0,100),
        bp ~ dnorm( 0 , 1 )
    ), data=new_d,
    warmup=1000 , iter=5000 , chains=4)

m10.13 <- map2stan(
    alist(
        total_tools ~ dpois( lambda ),
        log(lambda) <- a + bc*contact_high,
        a ~ dnorm(0,100),
        bc ~ dnorm( 0 , 1 )
    ), data=new_d,
    warmup=1000 , iter=5000 , chains=4)

# intercept only
m10.14 <- map2stan(
    alist(
        total_tools ~ dpois( lambda ),
        log(lambda) <- a,
        a ~ dnorm(0,100)
    ), data=new_d,
    warmup=1000 , iter=5000 , chains=4)

islands.compare <- compare(m10.10,m10.11,m10.12,m10.13,m10.14, m13.7)
```

```{r}
print(islands.compare)
```

The GP model fits much better than the other models.  It has fewer effective parameters, as well - only 4.

#Hard
###Problem 1
```{r}
data(bangladesh)
d <- bangladesh
d$district_id <- as.integer(as.factor(d$district))

m13.1h <- map2stan(
  alist(
    use.contraception <- dbinom(1, p),
    
    #Model block
    logit(p) <- A + BU*urban,
    A <- a + a_district[district_id],
    BU <- bu + bu_district[district_id],
    
    # adaptive priors
    c(a_district,bu_district)[district_id] ~ dmvnormNC(sigma, Rho),

    # fixed priors
    a ~ dnorm(0,10),
    bu ~ dnorm(0, 1),
    sigma ~ dcauchy(0,2),
    Rho ~ dlkjcorr(2)
    ),
  data=d,
  warmup=1000, iter=5000 , chains=1, cores = 1)

precis(m13.1h, pars = c("a", "bu", "a_district", "bu_district"), depth = 2)
```

```{r}
post <- extract.samples(m13.1h,)
a2 <- apply(post$a_district, 2, mean)
b2 <- apply(post$bu_district, 2, mean)
a1 <- mean(post$a)
b1 <- mean(post$bu)
plot(a2, b2, xlab = "intercept", ylab = "slope")
```

From the plot, we can see that slope and intercept are negatively correlated.  This implies that if a district has a low base rate of using contraception, being urban will have a greater increase on the likelihood of the use of contraception.  As we might expect, the base rate of contraception is much higher in urban areas than in rural areas.

```{r}
plot(x=rep(0, length(a2)), y=logistic(a1 + a2 + (b1+b2)*0), col = rangi2, xlim = c(0, 1), xlab = "Rural vs Urban", ylab = "P(Contraception)")
points(x=0, y = mean(logistic(a1 + a2 + (b1+b2)*0)), col = "red")
points(x=rep(1, length(a2)), y = logistic(a1 + a2 + (b1+b2)*1))
points(x=1, y = mean(logistic(a1 + a2 + (b1+b2)*1)), col = "red")


dens(post$Rho[,1, 2])
```

###Problem 2
```{r}
data("Oxboys")
d <- Oxboys

m13.2h <- map2stan(
  alist(
    height <- dnorm(mu, sigma),
    
    #Model block
    mu <- Alpha + Beta*age,
    Alpha <- a + a_subject[Subject],
    Beta <- b + b_subject[Subject],
    
    # adaptive priors
    c(a_subject,b_subject)[Subject] ~ dmvnormNC(sigma_parameter, Rho),

    # fixed priors
    a ~ dnorm(150,100),
    b ~ dnorm(7, 10),
    sigma ~ dcauchy(0,10),
    sigma_parameter ~ dcauchy(0,10),
    Rho ~ dlkjcorr(2)
    ),
  data=d,
  warmup=1000, iter=5000 , chains=4, cores = 4)



```

```{r}
precis(m13.2h, pars = c("a", "b", "a_subject", "b_subject", "sigma", "sigma_parameter", "Rho"), depth = 3)
post <- extract.samples(m13.2h,)
a2 <- apply(post$a_subject, 2, mean)
b2 <- apply(post$b_subject, 2, mean)
a1 <- mean(post$a)
b1 <- mean(post$b)
plot(a2, b2, xlab = "intercept", ylab = "slope")
dens(post$Rho[,1, 2])

```


A 1 unit increase in age increases hight by 6.51 (sd = .36).  The average height, assuming no age, is 149.33 cm (sd = 1.72).

###Problem 3
Since the correlation is positive, tall boys tend to get taller.

###Problem 4
```{r}
library(MASS)
set.seed(100)
sa <- mean(post$sigma_parameter[,1])
sb <- mean(post$sigma_parameter[,2])
rho <- mean(post$Rho[,1,2])
S  <- matrix(c(sa^2, sa*sb*rho, sb^2, sa*sb*rho), nrow = 2)
a.sim <- mvrnorm(n=10, mu = c(a1, b1), Sigma = S)

plot(x=0, y = a1 + b1, xlim=c(-1, 1))
for (i in 1:10){
  abline(a=a.sim[i,1], b =a.sim[i,2])
}
  
```
